[{"content":"The Inception üå† Since I began programming, I had been wanting a website of my own. Surely it is one of those things that initially gets someone to pursue programming. Not me though. I wanted a website to showcase my art, and programming projects and have a place to help grow my thoughts in. I wanted a very functional, yet lean website that didn\u0026rsquo;t pull my focus away from my projects and life.\nAfter much trial on different CMS like WordPress, Drupal and blogspot, I found them time-consuming to learn and start writing to. I had to create an account, choose and select my theming, tinker around till I could make it something that I sort of liked, and even then the UX was so messy, there were limited customizable options and etc. There was so much resistance I had to go through to write my blog that I eventually gave up.\nThe Conception üß† Fast forward two years of coding and writing documentation for it (yes, I do document :P), I theorised that if I could somehow get to write about my thoughts and projects in a Markdown format and get that published as a website, then maybe I can re-start my dream of my own website. An that\u0026rsquo;s when HUGO came into my life. It was perfect for my needs.\nI write in Markdown in VSCode -\u0026gt; Hugo takes it and builds an HTML website. A vague idea of what I want.\nIt also made me have total control on a style that I liked, pages I wanted to show and it was extremely fast to spin up and work in. No resistance. So I picked up a theme from the HUGO theme collection, set up my \u0026ldquo;blog-repo\u0026rdquo; and started writing into it. I did have to put my focus into learning how to tinker around in the theme to make it my own. Soon, I had a website that I felt \u0026lsquo;home-made\u0026rsquo;.\nHowever, I cannot expect people to come to my PC if they want to see my portfolio. Eventually you would want your website to be seen by other people, on their devices. That meant I had to consider hosting it somewhere. That somewhere was a no-brainer for me. I had learned from my Senior that Github can host code documentation from the repo itself, so I knew I could use that for hosting my personal website. Plus, it also gave me a free decent-sounding domain name, so another plus point. My only requirement now was to learn how to make Github look into my repo and host my website for me. This did not take much time to setup. Github has good documentation on how to setup github-pages for your repo. My process now looked like this:\nWrite blogpost-\u0026gt;build using Hugo, This generates a static website in the public/ directory-\u0026gt;rename it do docs/ cos Github only looked into either the root or docs/ directory -\u0026gt; commit and push to Github repo, Github now hosts my website by looking into docs/ folder -\u0026gt; \u0026ldquo;website is published\u0026rdquo;\nA convoluted approach to Operations.\nThe Correction üõ†Ô∏è Spinning up and setting it up was no problem, but I had no idea what was going on behind the scenes. And as an engineer, I had to know what was going on behind the scenes. This meant I had to learn DevOps( or just CI/CD, for now). And actually, I\u0026rsquo;m glad that I did this. Turns out my process of writing and publishing via Github had a completely, stupidly redundant step. I didn\u0026rsquo;t need to \u0026ldquo;build\u0026rdquo; my site. I didn\u0026rsquo;t need to go through the convoluted way on renaming my out folder. Github-Actions can take care of all that, if you let it to. I just had to setup a workflow that would constantly look for changes(commits) to my main branch, then build the public/ directory for me, and deploy, only the generated out directory, to another branch gh-pages. This allowed it to publish my website from the \u0026lsquo;/\u0026rsquo; root directory iin gh-pages branch rather than go searching for a docs/ directory. My mind was blown by the promise of technology. My entire process was now simplified:\nWrite Blog -\u0026gt; Commit and Push to Github; Github builds, moves and deploys website -\u0026gt; \u0026ldquo;Website is published\u0026rdquo;.\nA much simpler workflow.\nThe Conclusion ‚úÖ Now I have my own hand-built website where I can put my thoughts down with a peace of mind knowing that if anything went wrong I could fix it myself. I guess it is kind of like building your own little wooden cottage and painting it. Now, I have a digital home and I\u0026rsquo;m proud of it.\nA cozy wooden cottage.\nNow I can relax. Whew! \u0026hellip; or that\u0026rsquo;s what I wished for; but a small machine beside me beckoned me towards the dark side. \u0026ldquo;Host your website on me\u0026rdquo;, it hissed. What choice did I have, a mere computer-science-fanatical mortal? I reached over and took the card-sized machine in my hand and started thinking about it. My next challenge was to host my website on my RaspberryPi.\n","permalink":"https://siddharthchillale.github.io/blog/personal_website/personal_website/","summary":"The Inception üå† Since I began programming, I had been wanting a website of my own. Surely it is one of those things that initially gets someone to pursue programming. Not me though. I wanted a website to showcase my art, and programming projects and have a place to help grow my thoughts in. I wanted a very functional, yet lean website that didn\u0026rsquo;t pull my focus away from my projects and life.","title":"Turning my personal website dream into reality ft.Hugo"},{"content":"Motivation: There are some explanations on certain terminologies commonly seen whenever multithreading is brought up. These terminologies should be somewhat familiar to people who study computer science. However, even when I knew what they meant, I didn\u0026rsquo;t know how they interoperate with other \u0026ldquo;CS thingies\u0026rdquo;. For example, when one of my professor asked a question \u0026ldquo;If a computer just runs one thread at a time, is it necessary to ensure synchronization ?\u0026rdquo;, it befuddled me. I wondered if the question made any sense and why were we asked such a question. I also failed to understand how mutexes, locks, semaphores,etc were differ from each other and what does each of them bring to the table. This may make some sense to other people but it just challenged my understanding of multithreading and concurrent programming. One method to solidify our thoughts is to write about it. Hence this article. This article won\u0026rsquo;t provide with implementation details on each primitive. It is to throw light on when and where each primitive is useful, and not examples on how to write them in code. Understanding the above question opens up many aspects of concurrent programming. Here we will approach the synchronization primitives made available to use in the STL (C++ 20 and above).\nstd::mutex available in header \u0026lt;mutex\u0026gt; cppreference\nUsed generally for Exclusion rather than Ordering.\nAllows a thread to modify a shared variable.\nMust be released by the same thread that locks it. If not released, then there\u0026rsquo;s no way to allow other threads depending on it to run. Ultimately, the process (mostly the application) has to exit to regain control.\nOther threads who try to acquire the lock are blocked. What does it mean when a thread is said to be blocked ? The OS uses certain scheduling techniques(FCFS, round robin, MLFQS, etc) to schedule the time certain threads are run. When a thread X is said to be blocked, the thread doesn\u0026rsquo;t start executing even when the OS schedules it run. This is the reason, that even when a single thread is run at a time, it is necessary to ensure synchronization. There\u0026rsquo;s a possibility that when a thread is executing in its critical section, it is put to the back of the scheduling queue by the OS; and at the same time another thread can run its own critical section. There is no guarantee to make a thread run entirely on its own without being interrupted by the OS, but there are ways to increase the probability of this particular thread being run the most number of times 1. In any case, it is evident that even in a computer where a single thread is executed at a time, it is necessary to ensure synchronization. Remember, that issue of synchronization pops up when you are trying to make a program run by multiple threads and if these threads share some data between them.\nOther threads can avoid being blocked by using try_lock. This tries to own a mutex, but is not blocked if mutex is already taken. This allows the attempting thread to do something else while waiting for the lock to release. This means that when a thread is allowed to work by the OS scheduler, it\u0026rsquo;ll continue with some other work until the mutex is made available. This is useful, because when though a thread is blocked, the OS has alloted some time units for this thread to work in. This leads to the thread just waiting for the mutex to be free when it could do some other work. This effect is called \u0026ldquo;busy waiting\u0026rdquo;.\nNormally, mutexes are never used in their raw format, they are used along with a lock. C++ STL gives you multiple ways to use a mutex, through std::lock_guard, std::unique_lock and std::scoped_lock.\nIf the shared variables are simple primitive data items then C++ gives you the std::atomic. std::atomic can be used for cpp data types or even shared/weak pointers.\nstd::atomic cppreference\nstd::shared_mutex available in header \u0026lt;mutex\u0026gt;\nAllows multiple threads shared access if are only going to read from the shared data. Nothing much to it, it works similar to how mutexes are supposed to work except with more flexibility than just blocking any thread. Two types of locking mechanisms are provided : lock() and lock_shared() and their corresponding unlock() calls. lock() allows locking the access to just readers (i.e if you are reading only from the shared data store). lock_shared() allows locking the access to readers and writers (i.e if you are writing to the shared data store). Writers have exclusive access. When lock is acquired by writers, readers and other writers are blocked. lock had to be released by the same thread that acquires it. std::shared_mutex cppreference\nstd::scoped_lock/ std::lock_guard / std::unique_lock available in header \u0026lt;mutex\u0026gt;\nAll three provide a convenient way of owning a mutex through cpp‚Äôs RAII style. lock is released when execution goes out of scope..\nThis is not only to prevent developers\u0026rsquo; mistakes of forgetting to release the mutex but also to safeguard against exceptions.\nstd::unique_lock enables a mutex to be moved to another structure like std::condition_variable (more on this later); std::lock_guard provides simple RAII convenience but can only use one mutex. If nesting multiple mutexes is what you need then std::scoped_lock can hold multiple nested mutexes.\nstd::scoped_lock cppreference\nüí° Up until now, you only had to worry about Exclusion. i.e. two threads not changing the same shared variable. But what if you want more control over the order of threads executed. Suppose you have an application where you wish to display some data that is to be fetched from some an online repo, you have a function that does this fetching.\nNow you have heard about this new thing called multithreading and want to be very efficient with your time, so you assign another thread to fetch this data. You now have two threads, main and fetcher threads. you delegate the fetcher to fetch the data and allow your main to display the data. but how can the main display the data when the fetcher is made to sleep by the OS. The main thread doesn\u0026rsquo;t know what happened to the fetcher thread or how long it\u0026rsquo;ll take to get the data. While the fetcher is sleeping, the main thread displays nothing and probably exits. This is a problem because you have to make the main thread wait until the fetcher thread does its job.\nThis means you want some kind of mechanism to handle the order of execution to be such that main displays only when fetcher is done its job. This leads to the process of \u0026ldquo;Synchronizing\u0026rdquo; the threads. Following are some thingies made available by the C++ STL to make this power of synchronization available to programmers.\nCircumstances which benefit from a particular primitive.\nstd::condition_variable (aka cv) available in header \u0026lt;condition_variable\u0026gt;\nhas a queue that holds the waiting threads blocked due to some condition on the shared variable.\nThreads are blocked until another thread both modifies the shared condition and notifies the condition_variable.\nRequires a mutex to work. specifically std::unique_lock\u0026lt;std::mutex\u0026gt;. One condition variable should work only with single mutex. Whereas, multiple condition variables can share a single mutex. The cv queue is handled by the condition variable object, and has nothing to do with the mutex associated with the cv.\nTo block a thread on some condition, use cv.wait( lock, function ) . The function does the checking of a shared variable, and you require a lock to read this variable as it is shared between threads.\nThe Thread that wants to change the shared variable must acquire a mutex, modify the shared variable if it wants and then release the mutex, and notify cv. After notifying cv, the cv checks if the condition predicate is true and unblocks a waiting thread if it is true.\nThe example given in cpp reference doc is good and I\u0026rsquo;d recommend to give it a read an undestand what\u0026rsquo;s happening.\nstd::condition_variable cppreference\nstd::counting_semaphore, std::binary_semaphore (from C++ 20) available in header \u0026lt;semaphore\u0026gt;\nThink of a semaphore as a charging brick with multiple ports. The number of ports is analogous to the count in a counting semaphore. You can only attach a fixed number of devices to the ports. Other devices that wish to connect to these ports must wait for ports to open up.\nA counting semaphore has a mutex, a condition variable, and count c. In std::counting_semaphore one needs only to mention count c for constructing a c_semaphore.\nA binary semaphore is when the counting c is 1. Which allows only one thread to use the shared resource. This differs from a mutex in the sense of ownership. Mutex has to be released by the calling thread as it is owned by the calling thread. A binary_semapahore can be released by other threads even if they are not the owner of that lock.\nThis is similar to a cv in the sense that threads wait on the condition of c becoming \u0026gt; 0.\nUsed mainly for signalling/notifying rather than mutual exclusion. How ? By blocking other semaphores that are required by the other threads. A little rude maybe but it works. Semaphores are useful for having that particular order in execution of threads. Allowing access to a resource is done by a mutex/lock, making it efficient to determine (who among those allowed) gets the resource is done by a condition variable, and having a certain order in accessing this resource is done by a semaphore.\nstd::counting_semaphore, std::binary_semaphore cppreference\nstd::barriers (from C++ 20) available in header \u0026lt;barrier\u0026gt;\nUsed mainly for thread coordination. A barrier is a door which waits for a certain number of threads to come to it. Threads do not wait on some condition to become true. They simply wait till the barrier receives enough number of threads requesting for this particular resource.\nConsiders number of threads to wait for the barrier to release. Barrier is released when c number of threads are blocked on the wait() condition.\nCan be reused unlike std::latch\nstd::barriers cppreference\nstd::latch (from C++ 20) available in header \u0026lt;latch\u0026gt;\nUsed mainly for thread coordination. This is similar to std::barrier in sense that threads are made to wait on an external variable that is not used in their critical section. They wait until a latch is opened.\nA latch doesn‚Äôt require a thread to block itself to signal release of the barrier. Barrier is released when the a thread calls the count_down() function. Calling the count_down() function doesn‚Äôt block this thread.\nCalling thread can decrement latch multiple times.\nstd::latch cppreference\nstd::future (similar to Promise in js) This is similar to how Promise works in js. If a function is run asynchronously, it\u0026rsquo;s return value is stored as a std::future object. waiting on this future object blocks the thread until the value is made available. Otherwise, the thread is free to continue working its own logic.\nstd::future cppreference\nOne ways that I know how this could be done is by assigning priority to a thread. However, even this doesn\u0026rsquo;t guarantee that a thread will not be interrupted. This is because kernel threads may have a higher priority than threads of user programs and this cannot be controlled without kernel privileges.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://siddharthchillale.github.io/blog/multithreading_primitives/primitives/","summary":"Motivation: There are some explanations on certain terminologies commonly seen whenever multithreading is brought up. These terminologies should be somewhat familiar to people who study computer science. However, even when I knew what they meant, I didn\u0026rsquo;t know how they interoperate with other \u0026ldquo;CS thingies\u0026rdquo;. For example, when one of my professor asked a question \u0026ldquo;If a computer just runs one thread at a time, is it necessary to ensure synchronization ?","title":"Primitives in C++ for Multithreading"},{"content":"Design patterns in C++ are ways to structure code so as to maximise code reusability and versatility. Below are some of the keypoints of the different behavioural design patterns in C++ condensed into quick bites from a course on LinkedIn Learning.\nChain of Responsibility for series of nested handlers\nkinda like the middleware in express js\n1 2 3 4 class Handler{ virtual Handler* setNext (Handler* nextHandler) = 0; virtual Command* handle(Command* cmd)=0; }; Command To reduce coupling between classes that call one another.\nIntroduce a third middle class that reduces this coupling. e.g. A button class does not need to have a handle to the canvas. It can have a handle to the command class which in turn has a handle to the canvas it wants to run the command in.\nClearCanvasCommand \u0026mdash;\u0026gt; Command Interface (Inheritance) Button members =\u0026gt;[ Command* cmd] // takes a command type in constructor =\u0026gt;[ onclick() calls cmd-\u0026gt;execute()] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Command{ virtual void execute() = 0; }; class ClearCanvasCommand : Command{ Canvas* canvas; void execute() override{ canvas-\u0026gt;clearAll();\t} }; class AddShapeCommand : Command{ Canvas* canvas; Shape* shape; // \u0026lt;--constructor--\u0026gt; void execute() override { canvas-\u0026gt;addShape(shape); } }; class Button{ Command* command; // \u0026lt;--constructor--\u0026gt; virtual void click() { command-\u0026gt;execute(); } }; Mediator Reduce coupling between complex object dependencies.\nAllow an instance of mediator to facilitate communication between different objects. e.g. UI object has TextBox, CheckBox, ButtonElement\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Mediator{ virtual void mediate(); }; UserInterface : Mediator{ tb = new TextBox; cb = new CheckBox; bu = new ButtonElement; virtual void mediate(string eventstring) override{ switch(eventstring){ case textbox: tb-\u0026gt;doSomething(); case checkbox: cb-\u0026gt;doItsThing(); case buttonelement: bu-\u0026gt;click(); } }; Observer When we want only the objects that care about certain events to be notified of the change in occurence. This is a publisher and subscriber implementation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Subscriber{ virtual void notify(Publisher p, message m); }; class Publisher { virtual void subscribe(Subscriber s); virtual void unsubscribe(Subscriber s); virtual void publish(message m); }; class ChatGroup : Publisher{ vector\u0026lt;Subscribers\u0026gt; scp; // when a subscriber subscribes, add it to the vector scp; // when a subscriber unsubs, remove it from the vector scp; // in publish, call the notify() function of all subscribers in the vector scp void publish() override { for(auto\u0026amp; sub : scp){ sub-\u0026gt;notify(); } }; }; class ChatUser : Subscriber{ void notify() override {} ; // subscriber needs to implement this }; Interpreter When program needs the ability to interface with some sort of language outside its normal comprehension. e.g. understanding and executing mathematical expressions, understanding human languages like eng/ spanish or roman numerals.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Expression { virtual void evaluate() = 0; }; class OperationExpression : Expression{ std::string op; Expression* lhs; Expression* rhs; int evaluate(){ switch(op): case \u0026#34;+\u0026#34; : return lhs-\u0026gt;evaluate() + rhs-\u0026gt;evaluate(); case \u0026#34;-\u0026#34; : return lhs-\u0026gt;evaluate() - rhs-\u0026gt;evaluate(); case \u0026#34;*\u0026#34; : return lhs-\u0026gt;evaluate() * rhs-\u0026gt;evaluate(); case \u0026#34;/\u0026#34; : return lhs-\u0026gt;evaluate() / rhs-\u0026gt;evaluate(); case default: return 0; } }; class NumberExpression : Expression{ std::string op; void evaluate(){ return std::stoi(op); } }; State when a class can be in different state and needs to change behavior according to this state.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class State { State* next = nullptr; State(State* nextState) : next(nextState){}; virtual State* getNext() { return next; } // can be any function that operates differently based // on which state the object is in. for illustration, // getDescription is used virtual string getDescription() = 0; }; class PurchasedState : State { PurchasedState(State* nextState): State(nextState){} string getDescription() override{ return \u0026#34;Purchased\u0026#34;; } }; class InTransitState : State { InTransitState(State* nextState): State(nextState){} string getDescription() override{ return \u0026#34;InTransit\u0026#34;; } }; class DeliveredState : State { DeliveredState (State* nextState): State(nextState){} string getDescription() override{ return \u0026#34;Delivered\u0026#34;; } }; class Order{ State* s; Order (State* curState) : s(curState){}; void goToNext(){ s = s-\u0026gt;getNext(); } string getCurStateDescription{ if(!s) return \u0026#34;No states\u0026#34;; return s-\u0026gt;getDescription();\t} }; void main(){ DeliveredState* ds = new DeliveredState(nullptr); InTransitState* is = new InTransitState(ds); PurchasedState* ps = new PurchasedState(is); Order o(ps); o-\u0026gt;getDescription(); // returns \u0026#34;Purchased\u0026#34; o-\u0026gt;gotToNext(); o-\u0026gt;getDescription(); // returns \u0026#34;InTransit\u0026#34; o-\u0026gt;gotToNext(); o-\u0026gt;getDescription(); // returns \u0026#34;Delivered\u0026#34; o-\u0026gt;gotToNext(); o-\u0026gt;getDescription(); // returns \u0026#34;No states\u0026#34; } Strategy when program needs to choose between several different ways of doing things on the fly.\njust like state pattern, but no linking of different strategies to inter-convert them.\nTemplate Method Break entire process into a series of steps, represent each step as a method, and have each different process implement its own steps with their particular variations.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 GreetingCardTemplate{ virtual std::string heading(std::string\u0026amp; to){ return \u0026#34;Hello\u0026#34; + to; } virtual std::string ocassion(){ return \u0026#34;Thank you blah blah\u0026#34;; } virtual std::string footer(std::string\u0026amp; from){ return \u0026#34;Warm regards, \u0026#34; + from } std::string generate(std::string\u0026amp; to, std::string\u0026amp; from){ return heading(to) + ocassion() + footer(from); } }; BirthdayCardTemplate : GreetingCardTemplate{ std::string ocassion() override{ return \u0026#34;Happy Birthday\u0026#34;; } }; DiwaliCardTemplate : GreetingCardTemplate{ std::string ocassion() override{ return \u0026#34;Shubh Dipawali !!\u0026#34;; } virtual std::string footer(std::string\u0026amp; from){ return \u0026#34;From entire family, ü™î \u0026#34; + from } }; void main(){ DiwaliCardTemplate dct; BirthdayCardTemplate bct; string diwaliLetter = dct.generate(\u0026#34;Raju Chacha\u0026#34;, \u0026#34;Ambani\u0026#34;); string bdayLetter = bct.generate(\u0026#34;Ranchoddas Chanchad\u0026#34;, \u0026#34;Rastogi\u0026#34;); string bdayLetter = bct.generate(\u0026#34;Farhan\u0026#34;, \u0026#34;Rastogi\u0026#34;); } Visitor Add similar extraneous functionality to different classes.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class Visitor{ virtual\tvoid handleA(string name){}; virtual void handleB(datetime time){}; }; class DatabaseVisitor : Visitor{ void handleA(string name){ // save to a database }; void handleB(string action){ // save to a database }; }; class FileSysVisitor : Visitor{ void handleA(string name, int phone){ // save to a filesystem } void handleB(datetime timestamp, string action){ // save to filesystem } }; class A{ // Some members string name; int phone; void accept(Visitor\u0026amp; v); }; class B{ // Some members string action; datetime time; void accept(Visitor\u0026amp; v); }; void main(){ B bObj(action, currentTime); A aObj(\u0026#34;Rohan\u0026#34;, 9879879879); DatabaseVisitor dbv; FileSystemVisitor fsv; aObj.accept(dbv); // uses dbv to save aObj to database aObj.accept(fsv); // uses fsv to save aObj to filesystem //same for bObj bObj.accept(dbv); bObj.accept(fsv); } Iterator Pattern A way to travel through some container. This implements a next() function which enables the iteration of elements in the container.\nMemento allow some objects to save themselves in such a way that they can‚Äôt be modified until they say so. Maintain a vector of const class objects. imagine history of objects, push a const copy of every valid state of object in a vector/ stack. When undo is clicked, clear the current object and fill it with the latest copy from the vector/ stack.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Canvas; class CanvasMemento { friend class Canvas; const vector\u0026lt;Shape\u0026gt; shapes; CanvasMemento(vector\u0026lt;Shape\u0026gt; shapes) : shapes(shapes) {}; }; class Canvas { vector\u0026lt;Shape\u0026gt; shapes; vector\u0026lt;CanvasMemento*\u0026gt; oldStates; Canvas(){}; void addShapes(Shape new_shape){ oldStates.push_back(new CanvasMemento(shapes)); shapes.add(new_shape); }; void undo(){ shapes = oldStates.back()-\u0026gt;shapes; cm.shapes.pop_back(); }; }; Null-Object Instead of using raw null pointers to inititalize member variables, use a default object to initialise it to.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Node{ int val; virtual\tint getVal(){ return val; }; }; class ListNode : Node { Node* next = nullptr; int getNextVal(){ return next-\u0026gt;getVal(); // this will throw null object error, //if next is not assigned any object }; }; /////////////////////// class NullNode : Node{ int getNextVal() {return 0;} }; class ListNode : Node { Node* next = new NullNode; int getNextVal(){ return next-\u0026gt;getVal(); // this will not throw error. // The uninitialized object has val set to 0 }; }; üí° Composition vs Inheritance: Difference between having a parent class and having the same class as a member variable: Having a parent class means that you can handle the child class‚Äôs parent members by casting the handle of the child class to a pointer of parent class. Having an object of the parent class as a member doesn‚Äôt grant you this capability.\n","permalink":"https://siddharthchillale.github.io/blog/behavioural_design_pattern/beh_dp/","summary":"Design patterns in C++ are ways to structure code so as to maximise code reusability and versatility. Below are some of the keypoints of the different behavioural design patterns in C++ condensed into quick bites from a course on LinkedIn Learning.\nChain of Responsibility for series of nested handlers\nkinda like the middleware in express js\n1 2 3 4 class Handler{ virtual Handler* setNext (Handler* nextHandler) = 0; virtual Command* handle(Command* cmd)=0; }; Command To reduce coupling between classes that call one another.","title":"Behavioural Design Patterns in C++"},{"content":" Rendering Triangles through rasterization: Knowledge about the rendering pipeline process where triangles are converted into pixels on the screen by the graphics processing unit (GPU) through a technique called rasterization.\nModel loading with normals: How to load 3D models into the graphics memory of the GPU and incorporate normal vectors for lighting calculations.\nTexture Mapping: I gained experience with texture mapping, where images are applied to 3D models to enhance their realism.\nBlinn-Phong shading model: I gained knowledge about the Blinn-Phong shading model, which is a widely used lighting model in computer graphics. It takes into account surface properties such as ambient, diffuse, and specular lighting.\nDirect3D 11 Pipeline for rasterization: I learned about the Direct3D 11 Pipeline for rasterization, which includes three stages: Vertex Shader, Geometry Shader, and Pixel Shader. I gained an understanding of the role of each stage and how to write shader code.\nCamera motion control: I learned how to control the camera in a 3D scene, allowing the user to move and interact with the scene.\nWhat could be improved:\nNormal Mapping for textures: I could improve my skills by incorporating normal mapping, which adds fine details to textures, making surfaces appear more realistic.\nInstancing of models: I could improve my skills by learning how to efficiently render multiple instances of the same model, which is useful for creating large scenes with repeated objects.\nBetter Scene representation: I could improve my skills by working on creating better scene representation, which includes things like lighting and shadowing.\nExtensive control over model objects and scene lights: I could improve my skills by learning how to give more extensive control to the user over model objects and scene lights, providing a more interactive experience.\n","permalink":"https://siddharthchillale.github.io/projects/directx_adventures/dx_11_adv/","summary":"Rendering Triangles through rasterization: Knowledge about the rendering pipeline process where triangles are converted into pixels on the screen by the graphics processing unit (GPU) through a technique called rasterization.\nModel loading with normals: How to load 3D models into the graphics memory of the GPU and incorporate normal vectors for lighting calculations.\nTexture Mapping: I gained experience with texture mapping, where images are applied to 3D models to enhance their realism.","title":"My Journey into learning Direct3D 11"},{"content":"CUDA is a library for using Nvidia GPU cores for parallelising code, but it‚Äôs multi purpose and is not tailored only towards graphics applications. Hence, the API is low level and memory needs to be managed responsibly by the developer themselves. This makes the code a pain to write and the performance obtained is not much improvement over the software multithreading option (although I admit that this code is not a good bench mark for checking performance between CUDA and CPU parallelism).\nPrerequisites for this post : This post is for accelerating the raytracing in one weekend code using CUDA, which means you need to be familiar with the raytracer code used in \u0026ldquo;raytracing in one weeekend\u0026rdquo;. I mostly followed the excellent guide on Accelerating Raytracing using CUDA presented in Nvidia\u0026rsquo;s blog.\nwhat Cuda is expert at is talking to the GPU and allocating resources that can be used either by GPU or GPU and CPU both. Hence, setting up a Common Frame Buffer is important.\nSystem Properties:\nCuda 11.7 Visual Studio 2022 community edition Device Query Setup - Install the Cuda toolkit which comes with the Cuda library, debugger tools, optimization tools, Cuda c++ compiler and runtime library. To make the C++ code compatible with cuda compiler, the C++ file where the cuda code is used must have extension .cu. During the setup for Cuda in Visual Studio, I encountered some problems, the solutions to which are listed below.\nThere should be a CUDA C/C++ tab in the project properties. If not then go to Build Dependencies by right clicking on the Project in the Solution View, and make sure the CUDA 11.7 (or whatever version of CUDA you have) box is selected True. Also make sure that you have set your runtime lib to MT in Linker \u0026gt; Code Generation tab in Project Properties. When you build the code, the compiler may throw an error where VS cannot find CUDA 11.7rt. In that case, you need to find the device compatibility version by running the Device Query sample from the CUDA samples. In my case, my device is 5.0 compatible. Then, add \u0026ldquo;compute_50, sm_50\u0026rdquo; (50 because 5.0 in my case ) in CUDA C/C++ \u0026gt; Device \u0026gt; Code Generation tab in Project Properties. After following the above steps, the setup was done and my code built without errors. Now, I followed through with the raytracing with CUDA guide by Nivdia, and made the following observations\nUsing printf() in the CUDA segment crashed my PC entirely. It did not just crashed the raytracing software, but just straight hung up my laptop (I had to forcefully shut it down). My guess is that since control was not handed to CPU by the GPU implicitly when printf() was called inside the GPU segment of the CUDA code. This made me be wary about experimenting with CUDA as some crashes could potentially be non-recoverable. This observation is a followup on the previous that STL library cannot be used in CUDA code, which entails that STL smart pointers cannot be used which means you have to resort to passing objects via dumb pointers or even double pointers. Cuda provides dynamic memory allocation through cudaMalloc() and cudaFree() but this introduces a danger as it becomes the developer\u0026rsquo;s responsibilty to manage freeing of resources. Having to deal with these make the code messy to deal with. ","permalink":"https://siddharthchillale.github.io/projects/rt_cuda/rt_cuda/","summary":"My understanding from porting Raytracing to Cuda","title":"Cudafying ray tracing in one weekend"},{"content":"Hello, I wanted to learn more about raytracing. I had taken up a Computer Science course in university before which taught ray tracing. Now I did understand the theory of raytracing and the math behind it, but I didn\u0026rsquo;t completely understand how the codebase was structured. Luckily I came across Pete Shirley\u0026rsquo;s books on RayTracing, namely Ray Tracing in One Weekend, RayTracing the next week, and Ray Tracing the rest of your life.\nBelow is my journey through all three books:\nRay Tracing in One Weekend. This book goes through creating an image first and foremost through the method of ray tracing. The first two chapters were adding utility to the code (3 dimensional vector class, image output). The book has stuck with writing the output to std::cout and then redirecting it to a .ppm file. I didn\u0026rsquo;t find a lightweight software to open such files and opening it became a hassle. To make things easy, I stored the output in a memory buffer and then passed this buffer to a custom class FrameBuffer which saves it in a .tga format. Code is given below which I took from a youtube channel \u0026ldquo;nicebyte\u0026rdquo;. Images can also be stored in .png format using the stb_image.h, available on Github here: https://github.com/nothings/stb. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // FrameBuffer.h : for saving image to a tga image file // I\u0026#39;m not writing the entire class, but you can use this function. // m_buffer is the image buffer where the pixel colors are stored after raytracing is done void save(const char* filepath) const { // filepath is relative to the $WorkingDir // (directory where the vcproj file is stored) FILE* pFile = fopen(filepath, \u0026#34;wb\u0026#34;); //_ASSERT(f); putc(0, pFile); putc(0, pFile); putc(2, pFile); /* uncompressed RGB */ putc(0, pFile); putc(0, pFile); putc(0, pFile); putc(0, pFile); putc(0, pFile); putc(0, pFile); putc(0, pFile); /* X origin */ putc(0, pFile); putc(0, pFile); /* y origin */ putc((m_img_width \u0026amp; 0x00FF), pFile); putc((m_img_width \u0026amp; 0xFF00) / 256, pFile); putc((m_img_height \u0026amp; 0x00FF), pFile); putc((m_img_height \u0026amp; 0xFF00) / 256, pFile); putc(24, pFile); /* 24 bit bitmap */ putc(0, pFile); fwrite(m_buffer, channels, m_img_width * m_img_height, pFile); fclose(pFile); } Ray Tracing involves generating a ray which originates from the camera in the direction of the pixel on the screen. This ray may or may not intersect our geometry scene. It helps to imagine that the camera is on the world origin (0,0,0) and the screen is at an unit distance away in the direction the camera is facing. The camera follows the right hand coordinates system are such that the -Z axis is the direction where our camera is facing. The screen space is generally normalized to -1, 1 in X and Y axes, but it is not followed in this walkthrough.\nNow that we have a ray that shoots from the camera position in the direction of the pixel coordinate on the screen space, we follow that ray through the screen and onto our scene geometry which lies behind the screen. Here, we consider that our scene consists of only spheres. After figuring out the math that\u0026rsquo;s required for calculating the intersection between a ray and a sphere, we get two hit points on the sphere. Now, the code just returns one hitpoint which is hardcoded, but the hitpoint where the normal at the hit point makes a negative cosine with ray dir should be considered. When a hit occurs, information about the place where the hit occured is stored in a struct(or an object) and returned to the caller of the intersection. This information includes the distance of this hit point from the pixel, the normal at the point of hit, the parameter t used in the parametric representation of the ray, the material of the object that it hit, if the face it hit is a front face of not (can be determined by dot pdt too), etc. This information is defined as the developer sees fit (Now DirectX12 handles this differently but that\u0026rsquo;s a topic for another post).\nA glass sphere with double glass walls\nnow that we have information about the point that our ray hit, we can do calculations on how to color this pixel from whose ray we intersected an object in the scene. This can be either simple by looking up the material from a resource already created earlier and which is available to the CPU memory, or it can be complicated if the object surface is special like Metal, Dielectric or a combination/permutation of both. This also gets comlicated if the object itself is a volumetric object. In these speacial cases, knowledge of geomtery comes in handy to figure align=center out where the ray travels next. The rest of the chapters handle what types of material exists.\nThe above process generates a single ray from the camera to the scene. However, that seems limiting and not that helpful, since I\u0026rsquo;m only getting information of the object material itself and not how it interacts with light sources or even other objects. To go further, we generate rays that originate from our hit points and go further in different directions according to the laws of light physics on this material. These next rays will further branch out from their hit points to different directions to collect information about the next hit. It helps to understand as if smaller cameras originate from the hit points and look in another particular direction to see how the world looks from this new position and orientation. This gives rise to our raytracing function going recursive. Each time information about the hit point is receieved, the color is determined and an attenuated value is sent to the caller of the raytrace function.\nFor mirror and dielectrics(e.g. glass) figuring out resultant ray direction is fairly simple geometry since we know the incident ray direction and the normal at that point. We can calculate our resultant ray direction from this using the Snell\u0026rsquo;s Law. A nice approximation we can use instead of the Fresnel\u0026rsquo;s law for specular reflection is the Schlick\u0026rsquo;s approximation here.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // Material.h class Dielectric : public Material { public: Dielectric(double index_of_refraction): ir(index_of_refraction){} virtual bool scatter(const Ray\u0026amp; r_in, const hit_record\u0026amp; rec, Color\u0026amp; attenuation, Ray\u0026amp; scattered)const override { attenuation = Color(1, 1, 1); double refraction_ratio = rec.front_face ? (1.0 / ir) : ir; Vec3 unit_direction = unit_vector(r_in.get_direction()); auto cos_theta = fmin(dot(-unit_direction, rec.normal), 1.0); auto sin_theta = sqrt(1.0 - cos_theta * cos_theta); Vec3 resultant_raydir; if (refraction_ratio * sin_theta \u0026gt; 1.0 || reflectance(cos_theta, refraction_ratio)\u0026gt;random_double()) { resultant_raydir = reflect(unit_direction, rec.normal); } else { resultant_raydir = refract(unit_direction, rec.normal, refraction_ratio); } scattered = Ray(rec.p, resultant_raydir); return true; } private: double ir; private: static double reflectance(double cosine, double ref_idx) { // Schlick\u0026#39;s approximation auto r0 = (1 - ref_idx) / (1 + ref_idx); r0 = r0 * r0; return r0 + (1 - r0) * pow((1 - cosine), 5); } }; A glass sphere with double glass walls\nThe code digresses a bit to cover camera movement and adding an in-camera blur. A thin lens approximation is used for achieving blur. Translating and rotating camera\nIn-Camera depth of field by manipulating Aperture\nI have skipped over the details of the diffuse and specular materials here. This is because these topics are important topics that deserve more than a breif summary. However, I will not leave you shivering in the cold there. To make it rudimentarily simple, I summarise it thus : In a perfectly specular material, light is reflected perfectly. This is a mirror material and therefore we can generate the output ray direction easily. To generate a rough mirror/ metal look, we go for a not-so-perfect reflections. This means we sample a direction that is generated from a cone using the perfect reflection direction as the cone axis. For a diffuse material, the resultant ray direction can be from any direction from hit point. Thus we generate a random vector in a hemisphere aound the hit point and send out next ray there. Sphere with diffuse lighting\nNow each ray when it hits an object send the information of that object back to the caller of that particular ray. this leads to an infinite recursive call. Since we don\u0026rsquo;t want to crash our systems with filled out stack, we limit the recursive call to a limited number decided earlier.\nAt this point, we have a basic offline ray tracer which only generates image files. The program runs on a single processor and a single thread. As such, a simple to complex scene takes up ~(5 to 15)mins to generate a 500x500 image.\nTo read how we improve performance, go read my post on Ray Tracing: The Next Week.\nA link to my code is here: SiddharthChillale/Ray_Tracer\nlink to the original book is here : Ray Tracing in One Weekend‚ÄîThe Book Series\n","permalink":"https://siddharthchillale.github.io/projects/rt_one_weekend/rt_one_weekend/","summary":"My understanding from Ray Tracing in One Weekend.","title":"Ray Tracing in One Weekend"},{"content":"Ray tracing next week\nMotion Blur Basically, we open the camera shutter for some time duration from time t0 to time t1. In that time, the object is sampled at some random intervals and the aggregate of all these samples is added up to give the final image of the object. This results in the object rendered as blurred due to motion. While this effect can be achieved in screenspace, a way to achieve it in-camera is shown here, I believe, to introduce the idea of timed-rays (or rays having temporal existence). However, this implementation does have shortcomings though. Namely that this captures motion blur of objects that are directly hit by the camera rays, but motion blur is not captured in the reflections. I might have to present a good example to showcase this effect.\nBVH This is an important topic to accelerate ray-object intersections. Implemented properly, this can achieve better performance. As our application is single thread on CPU, the boost in the performance is negligible for our simple scenes. How BVH works is that we try to eliminate the most occuring case as fast as possible. The most occuring case being that the object is NOT hit by the ray. Till now, we send a ray through every pixel of the screen. This ray is check for intersection with spheres that collectively make up our scene. It seems rather unnecessary that we have to do the complicated ray-sphere intersection tests for every single ray. We can reduce the time-consuming math if we can somehow eliminate the cases where there is a 100% chance that the ray will not hit the sphere. This is achieved rather cleverly, by using bounding boxes. Axis Aligned Bounding Box or AABB tests are fast to calculate. How this goes is that we create a tight enveloping box around our primitives(spheres in this case) that is aligned wrt to the camera coordinate axis. (citation need for the last part). Calculating ray-box intersections is simple to implement as well as to calculate. . Now preparing one box per sphere is good, but if you have hundred spheres,, calculating hundred AABB tests throws us back to out original problem and things get slow again. To overcome this, boxes that are close to other boxes can be grouped up to create a larger box that envelopes the two AABBs, This generates a tree of bounding box heirarchies that becomes easier to calculate. When an intersection with a AABB occurs, the ray then checks if there is an intersection with any of its children, and recurses its way through to ultimately do ray-sphere intersection tests.\nSolid Texture Mapping We create a texture class. this is attached to an object\u0026rsquo;s material and acts as its base color. Here, we consider solid texturing, i.e. geometrically achieved textures rather than image look ups. Ray traced Spheres with checkerboard texture\nPerlin Noise We oversee the different types of random noise generation and create a noise texture. Ray traced Spheres with discrete noise\nRay traced Spheres with noise texture\n5. Image Texture Mapping\nWe looked at geometric textures, and now we can handle image texture data. We learn about uv mapping into an image.\nRectangles and Lights Here we come across the rendering equation that we were avoiding for so much time. Light/emissive surfaces are introduced. And the color function includes an approximation of the rendering equation. WE also create a scene of our sphere + rectangles primitives. There\u0026rsquo;s no ray-triangle intersection handled until now or even in the future abd so mesh-laoding is not implemented. Emissive sphere and rectangle\nInstances for rotation and translation how rotation and translation is handled is very unintuitive for me. now it makes sense when I relate it to rasterization. In rasterization, to handle object movement, we transform world transformation to camera space transformations. However, in raytracing , instead of applying transformations to objects we apply the inverse transfomations on the rays that hit the object. seems very unintuitive, but it makes sense if you think about it and do the math.\nVolumetric objects Volumetric objects can be thought of as materials where light enters the material and changes direction and intensity the further inside it is in that said material. Volumetric cuboids\nIf you missed the post previous to this one (Ray Tracing in One Weekend), read it here\nA link to my code is here: SiddharthChillale/Ray_Tracer\nlink to the original book is here : Ray Tracing in One Weekend‚ÄîThe Book Series\n","permalink":"https://siddharthchillale.github.io/projects/rt_next_week/rt_next_week/","summary":"My understanding from Ray Tracing: The Next Week.","title":"Ray Tracing: The Next Week"},{"content":"The Unreal Engine utilizes a rendering pipeline to generate high-quality visuals while maintaining optimal performance across all platforms. This pipeline supports both forward and deferred rendering, and the following overview is based on the \u0026ldquo;Rendering Kickstart\u0026rdquo; talk from the Unreal Engine Community Talks.\nBEFORE START OF RENDERING CPU on frame 0: does work on physics, networking, animations, etc. then passes on the work to GPU GPU is 2 frame behind CPU because of the pipelining in CPU. a thread passes data from CPU to GPU, GPU starts work, thread passes data from GPU to CPU. so GPU has to wait for data to come to it from CPU in frame0, and then wait for one more frame for data to process at CPU. At this point, position of every existing object is known. Frame 1 Computes the visibility of every object in the scene. what‚Äôs seen and what‚Äôs not seen by camera. this includes culing and types on culling. At this point, the visibility of every object is known. Occlusion/ visibility processes. Following processes are done in order given Distance Culling Frustum culling Precomputed visibility Visibility culling software occlusion mesh LODs hardware occlusion every mesh is an occluder for every other mesh RENDERING STARTS Frame 2: Rendering starts Early z pass: depth of objects are scanned. This is stored in the Scene Depth Buffer. BasePass: Drawcalls on all meshes (static and dynamic) + base materials + light maps (static lights and shadows). Initial rendering pass drawcalls of every mesh basepass on materials + lightmass { lightmaps (static lights and shadows, includes volumetric lights) } static volumetric light - dots are placed in the world in the volumetric lighting volume. light from dynamic lights is calculated and stored at these points. when an actor goes near these dynamic lights, it reads light values from the dots to compute it‚Äôs lighting. GBuffer¬†: is a set of 5 buffers that is maintained by cpu for every frame. Basepass renders itself from all it‚Äôs data to 5 buffers. SceneDepth Map Buffer A - World Normals Buffer B - Specular / Roughness/ Metallic together Buffer C - Base Color Buffer D - Additional Buffer E - Additional Dynamic Lighting and shadows: Direct and Indirect lights and shadows. are specialized and is very big. Direct Light IES profiles - This is the shape of the light cone. can be baked. Light function - can take an unreal material and blend light with it to cast dynamic lights. cannot be baked. Shadow dynamic shadows - regular shadows cast by for example, point lights. cascaded shadow maps - specifically outdoor environments. shadows need to disappear gradually with distance. distance field shadows - for an object, shadows of the part of obj nearer the surface are sharper than shadows of part of same obj further from the surface. raytraced shadows - full on raytracing Inset Shadows Per object shadows Contact shadows - fine detail shadows for when two objs are closer to each other. e.g. cable on a table Indirect Light Light propagation - Older real time GI system raytraced GI Shadow - First two are commonly used in games Capsule shadow - simplistic shadows (used for performance or scale) DFAO - distance field ambient occlusion raytraced AO Reflections: Reflection captures, planar reflections, screen space (SS) reflections, ray traced reflections reflection Captures - simplest and easiest. cube maps used to capture Planar Reflections - works on surfaces that are completely horizontal. not on walls Screen space reflections - don‚Äôt really understand how this works Raytraced reflections Additional¬†: Transparency Translucency Be aware of all the lighting models that are given for translucency. Lighting models: Volumetric Dir / nondir Vertex Dir / nondir surface translucency volume surface forward shading (nice but expensive) raytraced Atmospheric fog Sky atmosphere Exponential height fog - color fades out in distance and up. atmospheric fog - is complex than exponential height fog. enable volumetric fog. reads itself from lightmap and colors itself. also renders shadows too. also uses volumetric materials. PostProcessing: pretty much all layers (~75%) are enabled by default. not put in camera. they are put in a postprocess volume tonemapper - color and tint correction, contrast, sharpen Blooom - is multistate. blends all types of bloom together. standard convultion SSAO SSSSS - Screen Space Sub-Surface Scattering Screenspace GI - like photoshop Depth of Field Gaussian - cheap Bokeh - expensive but good Circle/Cinematic - good and cheap Exposure- min / max brightness Blendables - blend materials into pos process for advanced effects. happens in a pp volume Camera Effects grain, vignette, chromatic aberration, lens flare, dirtmark in a pp volume Final Frame: Performance GPU Profiler Statistics View modes Scalability : CVars - All the things that you can type in the console (‚Äô~‚Äô key). all things related to rendering starts as ‚Äúr.‚Äù things are temporary so feel free to experiment. can turn things on or off even while running the game. Using CVars for configs on different target platforms and devices. in-depth configs for every device can be set independently. Advices I haven\u0026rsquo;t completely understood how these are used, but I\u0026rsquo;ll list them below if ever required.\nShadow Rendering Managing tris amd draw calls Translucency Material Cost Forward / Mobile Baking Light Reflections Scalibilty Cvars/Show ","permalink":"https://siddharthchillale.github.io/blog/unreal_rendering/unreal_rendering/","summary":"A summary of the Rendering pipeline in Unreal Engine.","title":"Rendering Pipeline of Unreal Engine"},{"content":" git status\ngit log\n--stat : log with the change stats. git branch : shows all available branches.\n\u0026lt;branch_name\u0026gt; : creates a branch with name as \u0026lt;branch_name\u0026gt;. git ammend\n--amend -m \u0026ldquo;change the commit message\u0026rdquo; : changes commit message only --amend \u0026lt;nothing\u0026gt; : recommits with interactive editor for changing message. git checkout\n\u0026lt;branch_name\u0026gt; : switches to \u0026lt;branch_name\u0026gt; \u0026lt;file_name\u0026gt; : removes all recent changes for \u0026lt;file_name\u0026gt; \u0026lt;commit_hash\u0026gt; : pulls the commit with hash \u0026lt;commit_hash\u0026gt; in a \u0026ldquo;DETACHED HEAD\u0026rdquo; state. i.e. We are not on any branch but outside. aNy changes will be discarded in not made permanent. If changes are to made permanent, create a new branch. git diff\n\u0026lt;commit_hash_1\u0026gt; \u0026lt;commit_hash_2\u0026gt; : compares difference between commits with hashes \u0026lt;commit_hash_1\u0026gt; and \u0026lt;commit_hash_2\u0026gt;. git cherry-pick \u0026lt;commit_hash\u0026gt;\napplies changes from \u0026lt;commit_hash\u0026gt; to the current branch. git reset\n--soft \u0026lt;commit_hash\u0026gt; : return to commit with hash \u0026lt;commit_hash\u0026gt; with changes still in staging area. \u0026lt;commit_hash\u0026gt; : return to commit with hash \u0026lt;commit_hash\u0026gt; with changes in working directory and not staged. --hard \u0026lt;commit_hash\u0026gt; : return to commit with hash \u0026lt;commit_hash\u0026gt; with all the modifications deleted. Untracked files are left alone\u0026hellip; for getting rid of untracked files see git clean. git clean\n-df : removes untracked files(-f) and directories(-d). git reflog : shows all the log where a commit is referenced.\ngit revert\n\u0026lt;commit_hash\u0026gt; : undos the changes from commit with hash\u0026lt;commit_hash\u0026gt;. keeps git history clean. git fetch\n\u0026lt;remote_name/branch_name\u0026gt; : brings all changes from the remote repo to the local repo, but doesn\u0026rsquo;t merge with existing files. git merge\n\u0026lt;remote_name/branch_name\u0026gt; : Merging combines your local changes with changes made by others. git pull : A convenient shortcut for git fetch and git merge\n\u0026lt;remote_name/branch_name\u0026gt; : fetches all changes from the remote_repo \u0026lt;remote_name\u0026gt; to the local repo and merges the changes. If any merge conflicts, then do git merge --abort to take the branch to where it was before you pulled.\n","permalink":"https://siddharthchillale.github.io/blog/git-mistakes/git_mistakes/","summary":"git status\ngit log\n--stat : log with the change stats. git branch : shows all available branches.\n\u0026lt;branch_name\u0026gt; : creates a branch with name as \u0026lt;branch_name\u0026gt;. git ammend\n--amend -m \u0026ldquo;change the commit message\u0026rdquo; : changes commit message only --amend \u0026lt;nothing\u0026gt; : recommits with interactive editor for changing message. git checkout\n\u0026lt;branch_name\u0026gt; : switches to \u0026lt;branch_name\u0026gt; \u0026lt;file_name\u0026gt; : removes all recent changes for \u0026lt;file_name\u0026gt; \u0026lt;commit_hash\u0026gt; : pulls the commit with hash \u0026lt;commit_hash\u0026gt; in a \u0026ldquo;DETACHED HEAD\u0026rdquo; state.","title":"Handle git mistakes"},{"content":"I develop a simple path tracer that can render scenes with global illumination. The first two tasks focus on providing an efficient implementation of ray-scene geometry queries. In the third task (Pathtracer) I developed the ability to simulate light bounces around the scene. Specifically, indirect and direct illumination. The pathtracer renders COLLADA files.\nTask 1. Generating Camera Rays Task 2. Handling ray-triangle and ray-sphere intersections; Task 3: Path tracing Cornell Box with dodecahedron. 32 samples per pixel\nTask 1. Generating Camera Rays Steps:\nBy default, rays are generated from the left bottom of a pixel. Sample a random number from 0 to 1 to add to the pixel coordinates. Normalise with respect to the screen coordinates. Subtract 0.5 from both x and y coordinates to center the pixel. Generate ray to this new normalised location with the camera position as the origin of the ray. Calculate the sensor height and sensor width using the vertical field of view of the camera and using trigonometry. Take care to handle units carefully as field of view will be returned in degrees, whereas the trig functions take angles in radians. Scale the normalised screen coordinates with the sensor width and sensor height calculated from step 2. The ray created is generated with respect to the camera space. This ray needs to be converted back to world space by multiplying the ray direction with the camera to world transformation matrix (given in camera). figure align=center: Rays generated from camera. figure align=center: Camera with wider aperture for shallow depth of field Task 2. Handling ray-triangle and ray-sphere intersections Ray-Triangle Intersection: I used the Moller-Trumbore algorithm for Ray-triangle intersection.\nSteps:\nScotty3D function of ray-triangle intersection returns a Trace structure which contains the information around the point where the ray hit the triangle. This contains the information of the ray that hits that object, if the hit happens and the position where the hit happens. It also modifies the max valid bound of the ray, so that other later hits (such as hits that are behind the object) are ignored. The above figure align=center illustrates equations to calculate the ray-triangle intersection. Solving the equations we get a vector of 3-dimensions. The first two coordinates (u, v) are the barycentric coordinates w.r.t. the triangle (the beta and gamma). The last coordinate (t) is the distance along the ray where the intersection happens. Check if u \u0026gt;0 , v \u0026gt; 0 and 1 - (u+v) \u0026gt; 0 . This checks if the intersection happens inside the triangle. Check if the t value is within the valid range of dist_bounds.min and dist_bounds.max of the ray. If all the above conditions are true, we say the ray as hit the triangle. The position of hit is easily found by putting it in the ray equation. Ray-Sphere intersection Steps: Similar to ray-triangle intersection we have to fill the Trace structure. FRom the figure align=center, we can say that the ray intersects the sphere if the ray satisfies the equation of the sphere. Thus substitute the ray in sphere equation and find solutions two ideal solution t1 and t2. If the solutions are real numbers then we say that a hit has occurred. For this we can check early by checking the signature of the determinant of the equation. One thing to consider is that the equation assumes a unit sphere which is not the case in Scotty3D. Thus, radius needs to be taken into account for solving the equation. If there is a hit, update the max bound of the ray. Find the position where hit has occurred. Fill the trace structure and return it. Results from implementing ray-triangle and ray-sphere intersections are displayed below. figure align=center: Ray-sphere intersection on Cornell Box (cbox.dae) Ray-triangle intersection on model of Cow (cow.dae) Cornell Box with dodecahedron. 32 samples per pixel.\nTask 3. Path tracing Indirect illumination.\nSteps:\nRandomly sample a new ray direction from the BSDF distribution using BSDF::scatter(). Create a new world-space ray. New depth value is the previous depth-1. The bounds of the ray are set to (EPS_F, 2.0f) . We also offset the origin of the ray with direction * EPS_F. This is done to account for numerical precision. Doing so removes a lot of white dots that are visible in the scene otherwise. Call Pathtracer::trace() to get incoming light. We are only interested in the second component of this light which deals with the reflective component. Scale this by the bsdf distribution and the cos_theta. Since the sampling is done over a hemisphere, the pdf is constant at (1/ 2*Pi) Return the calculated radiance at that hit position. Direct illumination: Steps: Repeat the same steps as indirect illumination but with 2 differences. Call trace() to get incoming light. Since we are only interested in the emissive component we take the first component of incoming light. Now the depth value would set to 0 as we are only interested in the emissive component. Scale the radiance with cos_theta and the albedo of the material. Return the radiance calculated at the hit position. My implementation works fine for rendering triangles but has trouble rendering spheres. AS seen in figure align=center, even after 512 samples, spheres are not rendered in the image. Whereas, models with triangle primitives are rendered just fine. Plus, the current code works fine for area lights and point lights. I have included results from using both point lights and area lights Example of 1024 samples on cornel box.\nI followed this up with the Ray Tracing in One Weekend series to fill up void in my basic knowledge. You can follow it here : Ray Tracing in One Weekend\n","permalink":"https://siddharthchillale.github.io/projects/pathtracer/pathtracer/","summary":"A pathtracing course project.","title":"PathTracer: Global Illumination"},{"content":"\u0026ldquo;NIPS Papers Analysis\u0026rdquo;\nDescription- Discovering the hottest topics in machine learning by analysing NIPS papers using NLP libraries in python.\nGithub Link - SiddharthChillale/NIPS-papers-analyses\n\u0026ldquo;Risk and Returns: The Sharpe Ratio\u0026rdquo;\nDescription - Studying Sharpe ratio for understanding Risk and Return for Amazon and Facebook stocks through the Fiscal Year 2016.\nKaggle Link - siddalore/sharpe-ratio-study\n\u0026ldquo;Courier Management system\u0026rdquo;\nDescription - Built a courier managements system web application in Flask and database hosted with SQLite, Handled the Model and View of the app with database implementation.\nGithub Link - SiddharthChillale/courier-management-project\n\u0026ldquo;Python scraper for Goodreads\u0026rdquo;\nDescription- Scraping Goodreads for titles with images of any given author.\nGithub Link- SiddharthChillale/goodreads-scrapper\n\u0026ldquo;Command line application - Contact Book\u0026rdquo;\nDescription- A command line appliction for keeping your contact info using Click and sqlite.\nGithub Link - SiddharthChillale/cli-phonebook\n","permalink":"https://siddharthchillale.github.io/projects/small_projects/small_projects/","summary":"\u0026ldquo;NIPS Papers Analysis\u0026rdquo;\nDescription- Discovering the hottest topics in machine learning by analysing NIPS papers using NLP libraries in python.\nGithub Link - SiddharthChillale/NIPS-papers-analyses\n\u0026ldquo;Risk and Returns: The Sharpe Ratio\u0026rdquo;\nDescription - Studying Sharpe ratio for understanding Risk and Return for Amazon and Facebook stocks through the Fiscal Year 2016.\nKaggle Link - siddalore/sharpe-ratio-study\n\u0026ldquo;Courier Management system\u0026rdquo;\nDescription - Built a courier managements system web application in Flask and database hosted with SQLite, Handled the Model and View of the app with database implementation.","title":"Miscellaneous Projects"},{"content":"What to do to gain Experience Reorient your mindset with how the hiring manager or recruiters view you and not how you present yourself. How to know what the jobs require\nJob Descriptions Informational interview Linkedin Research BE REALISTIC Once you know what the requirements are\nLook for major roles and responsibilities. Prepare most IMPRESSIVE stories of your existing experiences. List out things you don\u0026rsquo;t know. VOLUNTEER Be ready to work for free\nGood for networking\nProfessional orgs ( Make it easy for them ).\nSmall Local Business ( They need tech support ).\nCreate your own job ( This is possible ).\nFREELANCING You earn stuff\nCan build your portfolio\nFiverr, Linked proFinder, UpWork\nStart a Blog : Can get opportunities for networking. : Shows that you are up to date with the world.\nTake additional projects from your current employer Join communities Shadow other employees LEARN HOW TO SELL YOURSELF Things to consider\nNetworking [ I use Linkedin and online forums/communities (discord | IRC) ] Personal Branding [ I use Blogs and Github ] Also read \u0026ndash; LinkedIn Tips\n","permalink":"https://siddharthchillale.github.io/blog/how-to-gain-experience/gain_exp/","summary":"What to do to gain Experience Reorient your mindset with how the hiring manager or recruiters view you and not how you present yourself. How to know what the jobs require\nJob Descriptions Informational interview Linkedin Research BE REALISTIC Once you know what the requirements are\nLook for major roles and responsibilities. Prepare most IMPRESSIVE stories of your existing experiences. List out things you don\u0026rsquo;t know. VOLUNTEER Be ready to work for free","title":"How to Gain Experience"},{"content":"LinkedIn Tips Summary : Market yourself with your elevator pitch Optimise LinkedIn profile components (summary and accomplishments) Describe effectively your work experience and projects Create connections participate in the content Your Aim ‚Äî Raise visibilty Important Points Write a polite and short introduction 1 2 3 4 5 6 7 Hi ___\u0026lt;insert name here\u0026gt;, I am Siddharth, a student pursuing Masters in computer science at University at Buffalo. I would love to hear more about your work at ___\u0026lt;insert their jobs here\u0026gt;. Would you be able to connect for a quick chat ? Thank you for your time ! Best, Siddharth. A LinkedIn summary is your elevator pitch about yourself. \u0026ldquo;Example summary\u0026rdquo; I\u0026rsquo;m a student pursing Masters in Computer Science at University at Buffalo, NY. My latest web app project is a dashboard ruuning data analysis and showcasing affected numbers during the COVID-19 crisis. Apart from my academics, I\u0026rsquo;m working on helping with the COVID research by running models on the gathered data.\nIn my free time, I continue improving my skills by applying my knowledge contesting in Kaggle competitions.\nSkills: Analysing Patterns in Data, Python, SQL/Bigquery.\nWork Experiences Order of events matters\nShould have atmost 3 bullet points expressing what you\u0026rsquo;ve done.\nAt least 1 chould demonstrate individual contribution.\nAt least 1 should communicate project result (with metrics, findings, etc)\nInclude relevant experience only\nGet more Connections\nGet 500+ connections Join groups Don\u0026rsquo;t bring your ego in the middle. Engage with LinkedIn content\nlike, share and post stuff Give recommendations and endorse people Informational Interview Questions for asking a new connection -\nWhat are some interesting aspects of your job? Any advice for someone looking to enter the field? How relevant was your education in getting this job? What other careers did you consider? How do you approach professional development in this field? What are you doing to keep your skills up-to-date? What do you wish you knew before you entered this field? Finally, make it a goal to leave a positive impression. Even if your new contact isn‚Äôt able to directly help you advance in your job search right now, you never know when they may think of you for a future opportunity.\n","permalink":"https://siddharthchillale.github.io/blog/linkedin-tips/linkedin_tips/","summary":"LinkedIn Tips Summary : Market yourself with your elevator pitch Optimise LinkedIn profile components (summary and accomplishments) Describe effectively your work experience and projects Create connections participate in the content Your Aim ‚Äî Raise visibilty Important Points Write a polite and short introduction 1 2 3 4 5 6 7 Hi ___\u0026lt;insert name here\u0026gt;, I am Siddharth, a student pursuing Masters in computer science at University at Buffalo. I would love to hear more about your work at ___\u0026lt;insert their jobs here\u0026gt;.","title":"LinkedIn Tips"},{"content":"It was around January 2019 when I heard of the rising cases of a deadly disease spreading in china. I was deep into studying data science and machine learning that time and lived and breathed on the Kaggle website. It was during this time that I found the dataset on the number of cases of COVID; people were already making kaggle notebooks on this data. I thought I had to jump on this train. Thus, I started a kaggle notebook with this dataset colected and made available by John Hopkins University. I quickly prototyped a working chart and started making all types of diagrams I could make from this data. \u0026ldquo;This is going to be a big deal\u0026rdquo; I thought. I was surprised, confused and delusionally delighted that such an infectious virus hadn\u0026rsquo;t made its way in India yet, especially considering that the country where it started was our neighbour. In fact, the US had reported early cases of COVID than India(though now I realise that it must have been started in India too but was not reported due to lack of intensive tesing done at that time). India had already set up travel restrictions to and fro China, so that might have delayed the spread.\nAnyway, I have digressed enough. I started making plans of converting this analytical notebook into a web app that could be statically hosted somewhere where it would automate the process of fetching data from JHU, processing the data and presenting it in a easy to understand dashboard. This is the start of that project which is now hosted on Heroku on this link.\nThe dashboard shows 5 diagrams.\nPie chart of the top 5 countries with highest total number of cases. Table showing the case data for the above top 5 countries. A graph of total confirmed covid cases against time. A graph of total death covid cases against time. A graph of new cases per total number of cases I used python for fetching and processing the data. Visualizing of data was done by the plotly library. The webapp is hosted on a heroku server.\nFinal\nA rolling mean of the number of new cases against the total number of cases.\nThe numbers had very extreme values due to inconsistency of data. Since we are focusing only on the trend, we do not require exact numbers. Therefore, a rolling mean of 5 values is implemented to smoothen out the irregularities. Initial\nA graph that plots new cases per total number of cases\nAdded a graph plotting new cases to existing cases. The Graph is number of New cases per Total number of cases. In order to understand exponential growths for diseases, we must not plot the growth against time (which is not so useful in prediction) ; but with the total existing numbers.1 A fall in the graph indicates lowering of new cases with total cases. i.e. We see that China and Korea are out of the danger of new cases. Italy and Spain are seeing small rates of controling the virus. Axes are plotted on logarithmic scale. Version 1.5 Date: March 23-27, 2020 Look of the website uplifted using plotly.\nUpdates\nAdded a Death Count Graph. Changed the theme of the dashboard to look more presentable. Added functionality of updating data in every refresh or at intervals of 6 hours. Version 1.0 Date: March 12-18, 2020 Pie chart of the top 5 countries with highest daily confirmed cases.\nGraph showing the daily count of total confirmed cases.\nImplemented a Confirmed Cases count graph. Implemented a New cases per day graph. Implemented a Pie chart for comparing proportions. Implemented a Table to give info on daily new cases. Deployed in production mode to Heroku. How To Tell If We\u0026rsquo;re Beating COVID-19 - minutephysics\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://siddharthchillale.github.io/projects/corona_dashboard/corona_dashboard/","summary":"Ay ! I developed a COVID-19 dashboard.","title":"Corona Dashboard devlog"},{"content":"","permalink":"https://siddharthchillale.github.io/tags/","summary":"","title":"Tags"}]